./pytest.ini:11:# E251 unexpected spaces around keyword / parameter equals
./tests/integration/test_discrete.py:11:from rl.memory import SequentialMemory, EpisodeParameterMemory
./tests/integration/test_discrete.py:81:    memory = EpisodeParameterMemory(limit=1000, window_length=1)
./tests/rl/agents/test_cem.py:12:from rl.memory import EpisodeParameterMemory
./tests/rl/agents/test_cem.py:23:    memory = EpisodeParameterMemory(limit=10, window_length=2)
./tests/rl/agents/test_cem.py:37:    memory = EpisodeParameterMemory(limit=10, window_length=2)
./docs/sources/agents/ddpg.md:12:The Deep Deterministic Policy Gradient (DDPG) agent is an off policy algorithm and can be thought of as DQN for continuous action spaces. It learns a policy (the actor) and a Q-function (the critic). The policy is deterministic and its parameters are updated based on applying the chain rule to the Q-function learnt (expected reward). The Q-function is updated based on the Bellman equation, as in Q learning.
Binary file ./rgb2ram/__pycache__/utils.cpython-36.pyc matches
./rgb2ram/utils.py:11:SOURCE_RAM_DIR = "../train_history/environments/ram"
./rgb2ram/utils.py:17:    Copy images and rams from ../train_history/environments to ./data/
./rgb2ram/utils.py:24:    if not os.path.exists(SOURCE_RGB_DIR) or not os.path.exists(SOURCE_RAM_DIR):
./rgb2ram/utils.py:31:    ram_files = []
./rgb2ram/utils.py:32:    for file in os.listdir(SOURCE_RAM_DIR):
./rgb2ram/utils.py:34:            ram_files.append(file)
./rgb2ram/utils.py:38:    assert(len(rgb_files) == len(ram_files))
./rgb2ram/utils.py:39:    data = list(zip(rgb_files, ram_files))
./rgb2ram/utils.py:50:    ram_dir = os.path.join(TRAIN_DIR, 'ram/')
./rgb2ram/utils.py:53:    if not os.path.exists(ram_dir):
./rgb2ram/utils.py:54:        os.makedirs(ram_dir)
./rgb2ram/utils.py:56:    for rgb, ram in train:
./rgb2ram/utils.py:58:        shutil.copyfile(os.path.join(SOURCE_RAM_DIR, ram), os.path.join(ram_dir, ram))
./rgb2ram/utils.py:61:    ram_dir = os.path.join(VAL_DIR, 'ram/')
./rgb2ram/utils.py:64:    if not os.path.exists(ram_dir):
./rgb2ram/utils.py:65:        os.makedirs(ram_dir)
./rgb2ram/utils.py:67:    for rgb, ram in validation:
./rgb2ram/utils.py:69:        shutil.copyfile(os.path.join(SOURCE_RAM_DIR, ram), os.path.join(ram_dir, ram))
./rgb2ram/utils.py:78:    ram_files = []
./rgb2ram/utils.py:79:    for file in os.listdir(os.path.join(TRAIN_DIR, "ram")):
./rgb2ram/utils.py:81:            y = read_image(os.path.join(TRAIN_DIR, "ram", file)).reshape(128)
./rgb2ram/utils.py:82:            ram_files.append(y)
./rgb2ram/utils.py:85:    train_ram = numpy.stack(ram_files, axis=0)
./rgb2ram/utils.py:86:    print("Train rgb matrix shape: {}, ram matrix shape: {}".format(train_rgb.shape, train_ram.shape))
./rgb2ram/utils.py:93:    ram_files = []
./rgb2ram/utils.py:94:    for file in os.listdir(os.path.join(VAL_DIR, "ram")):
./rgb2ram/utils.py:96:            y = read_image(os.path.join(VAL_DIR, "ram", file)).reshape(128)
./rgb2ram/utils.py:97:            ram_files.append(y)
./rgb2ram/utils.py:100:    val_ram = numpy.stack(ram_files, axis=0)
./rgb2ram/utils.py:101:    print("Validation rgb matrix shape: {}, ram matrix shape: {}".format(val_rgb.shape, val_ram.shape))
./rgb2ram/utils.py:103:    return train_rgb, train_ram, val_rgb, val_ram
./rgb2ram/train.py:29:# Network parameters
./README.md:5:The OpenAI Gym provides a set of Atari games environments with two input settings: screen frames in the form of RGB images, and RAM (a 256 byte array containing the state of the game). 
./README.md:6:There is no direct way to transform one input to another, since RAM potentially contains state information that is not 
./README.md:10:We will be using the openAI gym to get the Atari games (https://gym.openai.com/). One game we are considering trying transfer in is Amidar v0 to Amidar - ram v0.
./README.md:15:1. Generate matching pairs of RGB images and RAM states.
./README.md:17:2. Design and train a neural network (RGB2RAM) to map RGB images to RAM states.
./README.md:19:3. Train an RL model (RL-RAM) using RAM inputs.
./README.md:23:5. Transfer the RL-RAM model on the same task with RGB inputs by first converting the RGB inputs using RGB2RAM, then applying the RL-RAM model.
./README.md:28:7. (Extention) Repeat 1 & 2 for several different games so that RGB2RAM is a general mapping between RGB images and RAM states.
./README.md:30:RL-RAM model, we can transfer the RL-RAM model to our environment even we don't have the RAM inputs.
./README.md:38:1. `dqn_double.py` defines an dqn agent that records both RGB and RAM in callbacks during training.
./README.md:42:    TODO: change this to train a model on RAM.
./examples/cem_cartpole.py:9:from rl.memory import EpisodeParameterMemory
./examples/cem_cartpole.py:46:memory = EpisodeParameterMemory(limit=1000, window_length=1)
./examples/cem_cartpole.py:58:cem.save_weights('cem_{}_params.h5f'.format(ENV_NAME), overwrite=True)
./examples/dqn_atari.py:93:# If you want, you can experiment with the parameters or use a different policy. Another popular one
./examples/new_dqn_atari.py:2:Train dqn model on Atari games and save RGB screenshots and RAM during training.
./examples/new_dqn_atari.py:30:# todo: This processor is for training RGB inputs. Change to train on RAM inputs.
./examples/new_dqn_atari.py:104:# If you want, you can experiment with the parameters or use a different policy. Another popular one
./examples/new_dqn_atari.py:126:    # Use new_fit to save both RGB and RAM during training
Binary file ./.git/objects/pack/pack-62c8d6d7b04b5948c2e35cb4c5a290bea27fdfc2.pack matches
./.git/hooks/pre-rebase.sample:9:# The hook is called with the following parameters:
./.git/hooks/pre-push.sample:7:# This hook is called with the following parameters:
Binary file ./.git/index matches
./rl/memory.py:280:class EpisodeParameterMemory(Memory):
./rl/memory.py:282:        super(EpisodeParameterMemory, self).__init__(**kwargs)
./rl/memory.py:285:        self.params = RingBuffer(limit)
./rl/memory.py:290:        """Return a randomized batch of params and rewards
./rl/memory.py:296:            A list of params randomly selected and a list of associated rewards
./rl/memory.py:302:        batch_params = []
./rl/memory.py:305:            batch_params.append(self.params[idx])
./rl/memory.py:307:        return batch_params, batch_total_rewards
./rl/memory.py:318:        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)
./rl/memory.py:322:    def finalize_episode(self, params):
./rl/memory.py:323:        """Closes the current episode, sums up rewards and stores the parameters
./rl/memory.py:326:            params (object): Parameters associated with the episode to be stored and then retrieved back in sample()
./rl/memory.py:330:        self.params.append(params)
./rl/util.py:23:    params = dict([(k, v) for k, v in optimizer.get_config().items()])
./rl/util.py:26:        'config': params,
./rl/util.py:93:    def get_updates(self, params, loss):
./rl/util.py:94:        updates = self.optimizer.get_updates(params=params, loss=loss)
./rl/policy.py:299:    variance. The parameter C, which defaults to 1, can be used to correct for
./rl/policy.py:305:    on K actions with parameter C by using the BoltzmannQPolicy and setting
./rl/policy.py:309:        assert C > 0, "BoltzmannGumbelQPolicy C parameter must be > 0, not " + repr(C)
Binary file ./rl/agents/.new_dqn.py.swp matches
./rl/agents/cem.py:20:        # Parameters.
./rl/agents/cem.py:144:            params = self.get_weights_flat(self.model.get_weights())
./rl/agents/cem.py:145:            self.memory.finalize_episode(params)
./rl/agents/cem.py:148:                params, reward_totals = self.memory.sample(self.batch_size)
./rl/agents/cem.py:150:                best = np.vstack([params[i] for i in best_idx])
./rl/agents/cem.py:153:                    self.best_seen = (reward_totals[best_idx[-1]], params[best_idx[-1]])
./rl/agents/sarsa.py:86:        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.
./rl/agents/sarsa.py:140:            # Start by extracting the necessary parameters (we use a vectorized implementation).
./rl/agents/sarsa.py:152:            # Prepare and validate parameters.
Binary file ./rl/agents/__pycache__/cem.cpython-36.pyc matches
Binary file ./rl/agents/__pycache__/ddpg.cpython-36.pyc matches
Binary file ./rl/agents/__pycache__/dqn_double.cpython-36.pyc matches
Binary file ./rl/agents/__pycache__/new_dqn.cpython-36.pyc matches
./rl/agents/new_dqn.py:2:Dqn agent that records RGB screenshots and RAM during training.
./rl/agents/new_dqn.py:29:        Save both RGB images and RAM to /train_history/environments/
./rl/agents/new_dqn.py:80:        params = {
./rl/agents/new_dqn.py:83:        if hasattr(callbacks, 'set_params'):
./rl/agents/new_dqn.py:84:            callbacks.set_params(params)
./rl/agents/new_dqn.py:86:            callbacks._set_params(params)
./rl/agents/new_dqn.py:127:                            warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(nb_random_start_steps))
./rl/agents/new_dqn.py:151:                    observation_ram = deepcopy(env.unwrapped._get_ram())
./rl/agents/new_dqn.py:178:                    'observation_ram': observation_ram
./rl/agents/ddpg.py:54:        # Parameters.
./rl/agents/ddpg.py:140:            params=self.actor.trainable_weights, loss=-K.mean(combined_output))
./rl/agents/ddpg.py:249:            # Start by extracting the necessary parameters (we use a vectorized implementation).
./rl/agents/ddpg.py:262:            # Prepare and validate parameters.
./rl/agents/dqn.py:39:        # Parameters.
./rl/agents/dqn.py:113:        # Parameters.
./rl/agents/dqn.py:190:        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.
./rl/agents/dqn.py:257:            # Start by extracting the necessary parameters (we use a vectorized implementation).
./rl/agents/dqn.py:270:            # Prepare and validate parameters.
./rl/agents/dqn.py:566:        # Parameters.
./rl/agents/dqn.py:673:            # Start by extracting the necessary parameters (we use a vectorized implementation).
./rl/agents/dqn.py:686:            # Prepare and validate parameters.
./rl/core.py:110:        params = {
./rl/core.py:113:        if hasattr(callbacks, 'set_params'):
./rl/core.py:114:            callbacks.set_params(params)
./rl/core.py:116:            callbacks._set_params(params)
./rl/core.py:160:                            warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(
./rl/core.py:303:        params = {
./rl/core.py:306:        if hasattr(callbacks, 'set_params'):
./rl/core.py:307:            callbacks.set_params(params)
./rl/core.py:309:            callbacks._set_params(params)
./rl/core.py:344:                    warnings.warn('Env ended before {} random steps could be performed at the start. You should probably lower the `nb_max_start_steps` parameter.'.format(
./rl/core.py:680:        garbage collected or when the program exits.
Binary file ./rl/__pycache__/callbacks.cpython-36.pyc matches
Binary file ./rl/__pycache__/util.cpython-36.pyc matches
Binary file ./rl/__pycache__/policy.cpython-36.pyc matches
Binary file ./rl/__pycache__/memory.cpython-36.pyc matches
Binary file ./rl/__pycache__/core.cpython-36.pyc matches
./rl/callbacks.py:113:        print('Testing for {} episodes ...'.format(self.params['nb_episodes']))
./rl/callbacks.py:140:        self.observations_ram = {}
./rl/callbacks.py:150:        print('Training for {} steps ...'.format(self.params['nb_steps']))
./rl/callbacks.py:161:        self.observations_ram[episode] = []
./rl/callbacks.py:189:        nb_step_digits = str(int(np.ceil(np.log10(self.params['nb_steps']))) + 1)
./rl/callbacks.py:193:            'nb_steps': self.params['nb_steps'],
./rl/callbacks.py:217:                        ram = self.observations_ram[episode][i]
./rl/callbacks.py:218:                        ram = np.reshape(ram, (1, -1))
./rl/callbacks.py:219:                        save_rgb_array(ram, output_dir="./train_history/environments/ram/",
./rl/callbacks.py:225:        del self.observations_ram[episode]
./rl/callbacks.py:234:        self.observations_ram[episode].append(logs['observation_ram'])
./rl/callbacks.py:260:        print('Training for {} steps ...'.format(self.params['nb_steps']))
